---
title: "A Unified Approach for Ex Ante Policy Evaluation"
subtitle: "Evaluating Mechanisms for Universal Health Coverage"
author: John Graves, Vanderbilt University
abstract: ""
output:
  pdf_document: 
    keep_tex: yes
    number_sections: yes
  header_includes: 
     - \usepackage{floatrow}
     - \floatsetup[figure]{capposition=top}
     - \usepackage{bm}
  word_document:
    fig_caption: yes
    fig_height: 3.5
    fig_width: 6.5
    reference_docx: markdown-word-style.docx  
header-includes:
   - \usepackage{setspace}
   - \doublespacing
   - \usepackage{amsmath}
self_contained: no
always_allow_html: yes
bibliography: "../docs/01_ex-ante-evaluation_bibliography.bib"
urlcolor: blue
geometry: margin=1in
fontsize: 10pt
---

\centering
\raggedright

\singlespacing

I articulate an approach for quantifying the expected welfare consequences from economic and social policy decisions made under evidentiary uncertainty. Through an application to U.S. health reform policy, I demonstrate that embedding comparative welfare analyses within an ex ante model of counterfactual policy changes can both inform current policy adoption choices and help refine future research priorities. Specifically, using a generalized reduced-form model of health insurance reform, I simulate the effects of further expansion of coverage in the U.S. via in-kind public programs (e.g., Medicaid) or cash assistance to subsidize the purchase of private plans. Drawing on metamodeling and value-of-information (VOI) methods, I then evaluate the expected welfare loss from policy decisions based on current knowledge and given considerable structural, estimation and sampling uncertainty in model parameters. The findings provide meaningful guidance for both policymakers and researchers. Given current information, a targeted Medicaid expansion to remaining uninsured individuals less than 150% of the federal poverty line (FPL) yields the highest expected net welfare benefit provided society values a redistributive policy consistent with a marginal value of public funds (MVPF, or the ratio of policy benefits to costs) of 0.6 or lower. Above this threshold, a price linked subsidy ($25/month premium) to low income uninsured populations yields the largest expected welfare gain. However, the desirability of this policy erodes if society prefers policies with MVPF above 1. Not surprisingly, there is considerable uncertainty in these assessments. Assessment of the VOI reveals that to reduce this uncertainty, future research should focus on refining our understanding of (1) the insurance value of Medicaid, especially  to those not currently enrolled; (2) the role of informational and behavioral frictions in affecting willingness-to-pay parameters; (3) the incidence of uncompensated care; and (4) the health care costs of marginal enrollees in subsidized private plans. 

<!-- This study articulates an approach for quantifying the welfare loss from policy decisions made under evidentiary uncertainty. Specifically, I show that embedding comparative welfare evaluation within ex ante models of economic policy changes can both improve current policy decisions and help refine research priorities. Through an application of this method, I embed components for comparative welfare analysis within a generalized model of U.S. health insurance reform. This model draws on reduced form parameters to simulate the coverage and welfare effects of expansion via in-kind public programs (e.g., Medicaid) or cash assistance to subsidize the purchase of private plans. Using metamodeling techniques and value-of-information (VOI) methods, I evaluate the expected welfare loss from -->
<!-- policy decisions based on current knowledge and given considerable structural, estimation and sampling uncertainty in model parameters. These evaluations can both guide contemporary policy decisions and help refine future research priorities.  -->

\doublespacing

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction 

\setlength\parindent{24pt}

This study articulates an approach for quantifying the opportunity cost of social policy decisions made under evidentiary uncertainty. Through an application to U.S. health reform policy, I draw on recent advances in public finance and decision theory to embed comparative welfare analyses within a counterfactual policy projection model. By estimating the expected welfare loss from decisions based on current knowledge---which are possibly incorrect, given social preferences and incomplete information---I show how the approach can  inform current policy debates and help refine priorities for future research. 

The specific setting is one in which literature- and theory-based parameters inform counterfactual projections of social policy changes. This type of analysis---which I will call *ex ante policy evaluation*---is common worldwide. In the United States, models developed by the Congressional Budget Office (CBO) shape the trajectory and outcomes of federal policymaking. Outside of the U.S., entities like the National Institute of Health Care Excellence  commission models to determine coverage and reimbursement policy for government-sponsored health programs. Other public- and private-sector institutions have developed simulation models to project the consequences of changes to tax and transfer policy, education policy, and food policy, among others
[ @averyPoliciesPayoffsAddressing2019; @ballasSpatialMicrosimulationRural2006; @basuNutritionalPolicyChanges2013; @cordovaCOMPAREMicrosimulationModel2013; @decosterHowRegressiveAre2010; @feenbergIntroductionTAXSIMModel1993; @kristensenReducingChildhoodObesity2014; @mozaffarianCosteffectivenessFinancialIncentives2018; @smithFoodAccessHealth2006; @sutherlandEUROMODEuropeanUnion2013a; @szaboTAXSIMAgentBased2008]

Despite the important role played by ex ante evaluation in the policymaking process, scant formal attention has been paid to its theory, design and integration within the broader economic research enterprise.^[The one notable exception is health technology assessment, where rigorous standards for conduct, methods and reporting have been developed by the Panel on Cost-Effectiveness in Health and Medicine [@sandersRecommendationsConductMethodological2016; @weinsteinRecommendationsPanelCosteffectiveness1996].] To the extent there are explicit linkages, they usually take the form of simple, back-of-the-envelope counterfactual policy simulations that conclude empirical research manuscripts. Rarely do these exercises draw on a formal approach to comparative welfare analysis [@hendrenUnifiedWelfareAnalysis2019].  Even more rarely do they grapple rigorously with the role of estimation, sampling and structural model uncertainty in guiding policy recommendations and the direction of future research.

These and other shortcomings extend to formal policy simulation models as well.  For example, models often produce an array of welfare-relevant outputs and leave it to policymakers to weigh these separate factors when making decisions [@finkelsteinSubsidizingHealthInsurance2019; @finkelsteinValueMedicaidInterpreting2015a].^[This is particularly true in U.S. health policy, where federal policy decisions based on cost-effectiveness are prohibited through both legislation and administrative rulemaking.  But even absent a specific legislative or regulatory decree, the CBO and other modelers have generally avoided producing overall welfare assessments.] Moreover, while models often draw on standard economic theory and a shared evidence base, the underlying evidence is estimated with uncertainty (and possibly with bias) and is not always in uniform agreement. Models also differ in their underlying structure, data inputs and assumptions. Finally, the simulation process is often opaque, making it difficult for researchers to understand whether and how their work can inform modeling efforts. Put simply, few economists have a concrete sense of how their research can impact policy modeling done by the CBO and others---nor is it even clear whether current research is aligned around questions of highest importance to informing policy decisions.

<!-- Put simply, few economists have a concrete sense of how their research can impact policy modeling done by the Congressional Budget Office (CBO) and others---nor is it even clear whether current research is aligned around questions of highest importance to infroming policy decisions. -->

I propose a unified approach to ex ante policy evaluation that addresses many of the above shortcomings. The approach adds rigor to all stripes of ex ante evaluation. That is, deployment of the theories and methods discussed herein can add new and richer insights to counterfactual policy simulations that accompany empirical research studies, and to standalone policy simulation models used by the CBO and others. 

The primary theoretical contribution combines comparative welfare analysis based on the Marginal Value of Public Funds (MVPF; i.e., the ratio of incremental policy benefits to costs) with approaches for quantifying the welfare loss from policy decisions based on current (incomplete) information [@finkelsteinWelfareAnalysisMeetsa; @hendrenPolicyElasticity2016; @hendrenUnifiedWelfareAnalysis2019; @parmigianiDecisionTheoryPrinciples2009; @schlaiferAppliedStatisticalDecision1961]. Intuitively, at a given policy adoption threshold (e.g., a MVPF value of 0.8, below which a policy might be desirable but above which it may not), model parameter uncertainty may or may not affect policy adoption decisions. If decisions are insensitive to parameter variation based on bounds and uncertainty set by current knowledge, then the value of research to obtain further information on uncertain parameters is low---that is, it is not worth additional effort to reduce information uncertainty since the same policy decision would be made today as it would if we had better information. If policy adoption decisions are sensitive to uncertainty, however, I show that the MVPF can serve as a lens through which we can quantify the expected welfare loss from policy adoption choices based on current knowledge. I also show how to decompose model outputs to isolate how specific model parameters contribute to the overall value of information across a range of policy adoption thresholds. 

This study's second major contribution is an application of these methods to U.S. health reform policy. Specifically, I calibrate a discrete time and choice model of health insurance take-up to investigate further coverage expansion via in-kind government-sponsored programs (e.g., Medicaid) versus using subsidies to facilitate the purchase of private plans. The model itself relies on a relatively circumscribed set of literature-based reduced form parameters that capture coverage take-up decisions as well as valuation of in-kind vs. subsidized health insurance benefits [@graves_differenceindifference_2020; @finkelstein_subsidizing_2019; @finkelstein_value_2019]. As such, the underlying model fits within the recent "sufficient statistics" tradition in applied economics [@chetty_sufficient_2009]---and provides a template for researchers who wish to simulate the comparative welfare consequences of counterfactual changes to U.S. health insurance policy without the need for a detailed microsimulation model. 

<!-- The model is specifically parameterized to estimate the MVPF based on current evidence on the impact of public insurance expansion on coverage valuation and take-up, and on individuals’ valuation and take-up of subsidized private health insurance plans [@graves_differenceindifference_2020; @finkelstein_subsidizing_2019; @finkelstein_value_2019].^[] In an application, I examine the welfare consequences of further expansion of health insurance coverage to low-income populations via in-kind programs (e.g., Medicaid) versus using subsidies to facilitate the purchase of private plans.  -->

By specifying model parameters in terms of their expected value *and* their underlying uncertainty, I conduct large-scale sensitivity analyses by re-estimating model outcomes based on draws from the joint distribution of parameters. The parameter values and welfare estimates of this exercise are the primary inputs into a “metamodel”—that is, a regression model of how model outcomes vary as a function of input parameters. Metamodeling facilitates efficient estimation of the expected welfare loss from policy decisions based on current information. Given the nonlinearity of many policy simulation models, I explore different approaches to metamodeling including linear regression and artifical neural networks (ANNs). 

The results of this exercise yield valuable insights for policymaking and future research. First, given current information, a targeted Medicaid expansion to remaining uninsured individuals less than 150% of the federal povery line (FPL) yields the highest expected net welfare benefit provided society values a redistributive policy consistent with a MVPF of 0.6 or lower. Above this  threshold, a price linked subsidy ($25/month premium) for low income uninsured populations yields the largest expected welfare gain. However, the desirability of this policy erodes at MVPF thresholds above 1.

Not surprisingly, there is considerable uncertainty in these assessments. At MVPF adoption decision thresholds between 0.5 and 0.7, the parameter with greatest leverage is the insurance value of Medicaid. Current knowledge based on estimates derived from the Oregon Health Insurance Experiment may understate this parameter if Medicaid  provides an option value to those not currently eligible, but who could become eligible if they experience an unexpected health shock [@FinkelsteinValueMedicaidInterpreting2015]. Informational and behavioral frictions are also not fully captured in existing literature-based willingness-to-pay estimates, and these biases would also feed through this highly important parameter. However, if social preferences are consistent with adopting policies with an MVPF above 0.7, then future research should instead prioritize reducing information uncertainty in parameters that summarize the incidence of uncompensated care and the costs of marginal enrollees in private plans. 

The remainder of this paper is structured as follows. The next section lays out the theoretical foundations for summarizing the benefits, costs, and the expected welfare loss from policy decisions based on current information. This foundation facilitates a shared estimation framework for quantifying the value of information that applies to both empirical research studies and large scale policy microsimulation models.  Section TK outlines the assumptions and structure of the model used in the application. A methods section then follows with specifics on estimation of the value of information based on metamodeling. A results section summarizes specific insights based on the application, and a final section concludes. 


# Theory

We begin by defining summary measures of policy benefits $W(\boldsymbol{\pi},\alpha)$ and costs $C(\boldsymbol{\pi},\alpha)$ as outputs from either an empirical research study or a microsimulation model. These measures are defined for each policy strategy $\alpha \in \boldsymbol{\alpha}$ and based on model parameters $\boldsymbol{\pi}$.

Summary measures of benefits and costs are the key ingredients for the Marginal Value of Public Funds (MPVF) developed in a series of studies by Hendren [@hendren_unified_2019; @Hendrenpolicyelasticity2016]. In its most basic form the MVPF is the ratio of policy benefits to costs:

\[
MVPF(\boldsymbol{\boldsymbol{\pi},\alpha}) = \frac{W(\boldsymbol{\pi},\alpha)}{C(\boldsymbol{\pi},\alpha)}
\]

The MVPF measures the marginal value of an additional dollar spent on a policy. That is, the MVPF quantifies how the welfare benefits accrued by implementing the policy compare to the costs of adopting it.  These costs could be mechanical (e.g., the dollar value of a subsidy or cash transfer) and/or the result of economic frictions brought about through policy implementation (e.g., behavioral changes that result in changes in labor force participation, tax revenue, etc.). From a normative standpoint, however, the MVPF itself is agnostic: it simply measures the ratio of benefits to costs and does not make affirmative statements about whether a policy is “worth it.” 

To assess the desirability of policy adoption is useful to consider a benchmark threshold ($\lambda = MVPF_{bench}$) summarizing society’s willingness to implement a policy. This threshold value could simply be based on a MVPF of 1 or, if society values some redistributive consequence of the policy, could be set based on a value less than one.  For example, @finkelstein_subsidizing_2019 make comparative assessments of health insurance subsidization policies by specifying a social welfare function over Constant Relative Risk Aversion (CRRA) utility and a defined coefficient of risk aversion. This results in $\lambda =  0.2$. But researchers do not necessarily have to specify the structure of the social welfare function to define a decision-making benchmark. A value tied to an existing policy with strong social support could also suffice. For instance, Finkelstein, Hendren and Shepard (2018) also consider a benchmark ($\lambda = 0.88$) based on the MVPF of the Earned Income Tax Credit (EITC)---a popular means-tested cash transfer program. Finally, Hendren (2019) argues for the use of efficient welfare weights that project the welfare costs and benefits of any specific policy into the MVPF of a tax transfer between the affected populations. 

We next define the **net welfare benefit**, or the policy's benefits net of its costs scaled by $\lambda$

\[
NWB(\boldsymbol{\pi},\alpha,\lambda) = W(\boldsymbol{\pi},\alpha) - \lambda \cdot C(\boldsymbol{\pi},\alpha) 
\]
Intuitively, policies where $NWB \geq 0$ indicate situations where the MVPF is equal to or less than $\lambda$.

## The Opportunity Cost of Imperfect Information

It is important to emphasize that this study does not take a stance on the specific value of $\lambda$. Rather, both the theory and results emphasize that the utility of reducing information uncertainty in policy evaluation may, in many applications, vary over a range of values for $\lambda$. This is readily apparent by noting that the NWB varies as a function of $\lambda$. 

Relatedly, NWB also depends on model parameters $\boldsymbol{\pi}$, which are often estimated (or assumed) with uncertainty. Uncertainty could derive from sampling or estimation error, or because the parameter values are unexplored in the literature and an educated guess must be made. Alternatively, uncertainty could derive from potential heterogeneity in model parameters across welfare-relevant dimensions (e.g., income, education, geography) or there could be uncertainty around whether literature-based parameters (e.g., elasticities) can reasonably extrapolate to the setting, context and structure of the policy question at hand.

Uncertainty in the "true" value of $NWB$ means that a decision made based on a fixed set of parameter values could be incorrect for a given value of $\lambda$. That is, we might conclude that a policy either does or does not pass a cost-benefit test, but if we had more information we would come to the opposite conclusion. In short, given social preferences, policy adoption decisions based on current information often come with an opportunity cost of making the wrong decision. 

We next define the expected welfare gain that would accrue with perfect information on uncertain parameters. Without considering uncertainty, our policy decision is based on choosing the policy strategy that maximizes NWB with  parameters centered on their expected value, i.e., $\max_{\boldsymbol{\alpha}} E_{\boldsymbol{\pi}}[NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda)]$. Imagine, however, that we could implement a hypothetical study that eliminates all uncertainty, such that the policy adoption decision is based on $\max_{\boldsymbol{\alpha}} NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda)$. 

In practice, we may not know parameters with certainty but we can specify their distribution. In that case we can take the expectation over $\boldsymbol{\pi}$ so that with perfect information the expected (net) welfare gain is expressed as $E_{\boldsymbol{\pi}} \big [ \max_{\boldsymbol{\alpha}} NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) \big ]$.

Decision theory defines the difference between these two quantities as the *expected value of perfect information* (EVPI)---that is, it is the expected net welfare gain from a hypothetical study that could eliminate all model parameter uncertainty [@schlaifer_applied_1961; @parmigiani_decision_2009; @claxton_using_2006]:

\begin{equation}
\label{eq:evpi}
EVPI(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) =  E_{\boldsymbol{\pi}} \big [ \max_{\boldsymbol{\alpha}} NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) \big ] - \max_{\boldsymbol{\alpha}} E_{\boldsymbol{\pi}}[NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda)]
\end{equation}

Note that the second term in equation \ref{eq:evpi} is simply the expected net welfare benefit of the dominant policy strategy when parameters are centered on their expected value. We will call this strategy $\alpha^*$. We can therefore re-write the above expression as:

\[
EVPI(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) =  E_{\boldsymbol{\pi}} \big [ \max_{\boldsymbol{\alpha}} NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) \big ] -  E_{\boldsymbol{\pi}}[NWB(\alpha^*,\boldsymbol{\pi},\lambda)]
\]
rearranging gives us 
\[
=  E_{\boldsymbol{\pi}} \big [ \max_{\boldsymbol{\alpha}} \overbrace{ NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda)  -  NWB(\alpha^*,\boldsymbol{\pi},\lambda)} ^{L(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda)} \big ] 
\]

\[
= E_{\boldsymbol{\pi}} \big [\max_{\boldsymbol{\alpha}}  L(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) \big ]
\]
Where $L(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda)$ is a loss function specifying the welfare loss from adopting a given strategy relative to the NWB of the dominant strategy selected using parameters centered at their expected value (i.e., $\alpha^*$) and for a given value of $\lambda$. 

## Graphical Intuition for the EVPI 

Figure \ref{fig_voi_intuition} provides graphical intuition for the value of information as conveyed through the EVPI. Each scenario plots (as a solid black point) the estimated welfare benefit and cost outcomes from the "baseline" model run (i.e., with all parameters centered at their expected value). The grey points correspond to different model realizations under draws of the parameters from their joint distribution. This "cloud" of points visualizes the degree to which welfare cost and benefit outputs are sensitive to the particular values of parameters used. 

As can be seen in the figure, the value of information from reducing parameter uncertainty depends on how we define $\lambda$. In each plot, the slope of a ray from the origin has slope $1/\lambda$. Thus, points below each line correspond to combinations of policy benefits and costs that pass the defined cost-benefit test, while those above do not pass the test. 

As seen in Scenario A, when $\lambda$ is low (i.e., $\lambda_1$) there little value in obtaining new information: the same decision (to adopt the policy) would be made even if we allowed the model parameters to vary over their entire joint distribution. By comparison, at higher thresholds ($\lambda_2$)  the value of information is high: the "cloud" of points straddles the threshold line, indicating that different decisions would be made across a series of parameter draws.  

By comparison, in Scenario B there is still variation in modeled costs and benefits but no variation in decisions at either $\lambda_1$ or $\lambda_2$. Therefore, the value of information is low in both cases. Mathematically, the situation depicted by Scenario B amounts to the loss function $L(\boldsymbol{\alpha, \pi,\lambda})$ having a value of 0 at each realization of our model (and given the choice of $\lambda_1$ or $\lambda_2$): even under alternative draws of $\boldsymbol{\pi}$, we always choose the same dominant strategy ($\alpha^*$), so there is  no opportunity cost to making the wrong policy adoption decision.


```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_voi_intuition}Intuition for Value of Information Theory"}
knitr::include_graphics(here::here("./figures/03_voi.png"))
```

## Estimating the Value of Information for Parameters of Interest

It is often of interest to isolate the contribution of specific parameters (or sets of parameters) to the overall expected welfare loss from decisions based on current information [@strong_estimating_2014; @schlaifer_applied_1961; @jalal_gaussian_2018]. In that case, suppose we could obtain perfect information on the parameter(s) of interest  $\boldsymbol{\pi}_i$. The optimal policy decision would then be based on the policy alternative with the highest NWB after averaging over the conditional distribution of remaining parameters $\boldsymbol{\pi}_{-i}$. 

\[
\max_{\boldsymbol{\alpha}} E_{\boldsymbol{\pi_{-i}}|\boldsymbol{\pi_i}}NWB(\boldsymbol{\alpha},\boldsymbol{\pi},\lambda) 
\]
However, since $\boldsymbol{\pi}_i$ remains unknown, we we must take the expectation over current information: 

\[
E_{\boldsymbol{\pi_i}} \big [  \max_{\boldsymbol{\alpha}} E_{\boldsymbol{\pi_{-i}}|\boldsymbol{\pi_i}}NWB(\boldsymbol{\alpha},\boldsymbol{\pi}_i,\boldsymbol{\pi}_{-i},\lambda) \big ]
\]

Echoing equation \ref{eq:evpi} above, the *expected value of partial perfect information* (EVPPI) is the difference between this quantity and the expected net welfare benefit of the dominant policy strategy [@strong_estimating_2014; @schlaifer_applied_1961] : 

\begin{equation}
\label{eq:evppi}
EVPPI(\boldsymbol{\pi}_i,\lambda) = E_{\boldsymbol{\pi_i}} \big [  \max_{\boldsymbol{\alpha}} E_{\boldsymbol{\pi_{-i}}|\boldsymbol{\pi_i}}NWB(\boldsymbol{\alpha},\boldsymbol{\pi}_i,\boldsymbol{\pi}_{-i},\lambda) \big ] - 
E_{\boldsymbol{\pi}}[NWB(\alpha^*,\boldsymbol{\pi},\lambda)]
\end{equation}

In practice, we can estimate the EVPI and the EVPPI by conducting large scale probabilistic sensitivity analyses [@strong_estimating_2014]. To do this, we specify each parameter by its prior distribution and then re-estimate the model $K$ times after taking draws from the joint distribution of parameters. For instance, a model parameter based on a regression coefficient might be specified with a normal distribution centered on the coefficient value and with standard deviation set to the estimated standard error. Alternatively, parameters based on judgment calls might enter the model centered on some value, but with a diffuse (e.g., uniform) prior spread over a plausible range. Finally, it may be the case that parameters co-vary. In that case, we would specify the joint distribution of parameters (e.g., using a multivariate normal distribution centered on estimated regression model coefficients and with variance-covariance matrix as estimated by the same model).

Suppose we construct a $K$-sized sample of model outputs from a Monte Carlo-based sensitivity exercise. That is, we draw sets of parameters $\boldsymbol{\pi}^{(1)},\ldots,\boldsymbol{\pi}^{(K)}$ and generate a $K$-sized vector of net welfare benefits for each policy alternative modeled (i.e., $NWB(\boldsymbol{\alpha},\boldsymbol{\pi}^{(1)},\lambda), \ldots, NWB(\boldsymbol{\alpha},\boldsymbol{\pi}^{(K)},\lambda)$. 

We can estimate second component of equation \ref{eq:evppi} as

\begin{equation}
\label{eq:evppi_est_2}
\max_{\boldsymbol{\alpha}} \frac{1}{K} \sum_{k=1}^K NWB(\boldsymbol{\alpha},\boldsymbol{\pi}^{(k)},\lambda)
\end{equation}

Estimation of the conditional expectation in the first term in equation \ref{eq:evppi} is less straightforward. Borrowing from the approach in Strong, Oakley and Brennan (2014) we can express model outputs for this term as the sum of the conditional expectation plus a mean-zero error term:

\begin{equation}
\label{eq:evppi_est_1_1}
NWB(\boldsymbol{\alpha},\boldsymbol{\pi^{(k)}},\lambda)  =  E_{\boldsymbol{\pi_{-i}}|\boldsymbol{\pi_i}=\boldsymbol{\pi_i}^{(k)}}NWB(\boldsymbol{\alpha},\boldsymbol{\pi}_i^{(k)},\boldsymbol{\pi}_{-i},\lambda)  + \epsilon^{(k)}
\end{equation}
In addition, the expectation in equation \ref{eq:evppi_est_1_1} can be thought of in terms of an unknown function $g(\cdot)$ of $\boldsymbol{\pi_i}$:

\begin{equation}
\label{eq:evppi_est_1}
NWB(\boldsymbol{\alpha},\boldsymbol{\pi^{(k)}},\lambda)  = g(\boldsymbol{\alpha},\boldsymbol{\pi}_i^{(k)},\lambda) + \epsilon^{(k)}
\end{equation}

Based on equation \ref{eq:evppi_est_1}, we can estimate the conditional expectation in terms of a "metamodel," or a regression model predicting how the net welfare benefit for a particular policy varies with unknown parameters of interest $\boldsymbol{\pi}_i$. For example, a linear metamodel might specify the function $g(\cdot)$ as a standard regression equation. Alternatively, we might not wish to impose a functional form and instead estimate $g(\cdot)$ nonparametrically. Finally, we might suspect the underlying model has important nonlinearities in the parameters of interest, in which case we can appeal to machine learning methods to estimate $g(\cdot)$. We will explore these various approaches in our modeling application to U.S. health policy in section TK below.  

# Application: Modeling Approaches for (Near) Universal Coverage

We next turn to an application for projecting the welfare and coverage impacts of policies to achieve near- or fully-universal health insurance coverage in the U.S. The model and its results provide two distinct contributions. First, the model provides a proof-of-concept for embedding comparative welfare assessments and estimating the value of information within a counterfactual policy simulation model. In that sense, the modeling framework is a natural extension of Hendren and Sprung-Keyser (2020), who focus on estimation of the MVPF and the value of information from reducing sampling uncertainty in ex post policy evaluations. 

Second, the model is motivated by utility maximization theory but relies on a relatively circumscribed set of reduced form parameters. As such, the model fits within the recent "sufficient statistics" tradition in applied economics [@chetty_sufficient_2009]. Because of its simplicity and portability, the model provides a template for researchers who wish to draw on their own reduced form estimates to simulate counterfactual changes to U.S. health  policy  without the need for a detailed microsimulation model. Finally, I also discuss below how the model provides a useful intersection point between reduced form (elasticity-based) modeling approaches common in the applied health economics literature, and formal large-scale microsimulation models as used by the CBO and other research organizations.

## Discrete Choice Foundations

Consider a model of insurance choice among $J$ alternatives (including the choice not to insure). Define $U_{itj}$ as the utility for choice unit $i$ from selecting choice $j$ at time $t$.

\begin{equation}
\label{eq:utility_1}
U_{itj} = V(\mathbf{x_{itj}}, \mathbf{z_i})+ \epsilon_{itj}
\end{equation}

\noindent where $\mathbf{x_{itj}}$ is a vector of time-varying attributes of the $J$ choices and the health insurance unit (HIU), or the collection of related family members who could enroll under the same plan. Utility also depends on fixed attributes of the HIU ($\mathbf{z_i}$),  and an unobservable component $\epsilon_{itj}$.

For HIU $i$, the choice of insurance $y_{it}$ is based on maximizing utility across the $J$ alternatives at time $t$:

\[
y_{it} = {\arg \max}_j [U_{itj}, j = 1, \dots, J]
\]

We next define a function $B(\cdot)$ mapping utility from choice $j$ to $r_{ij} = P(y_{it} = j)$, the probability of individual $i$ selecting choice $j$. ^[As will become important in the next section, we can also think of similar choice probabilities at some time period $t$ conditional on the choice at time $t-1$: $P(y_{it} = s | y_{i,t-1}=r)$.] If the error terms $\epsilon_{ij}$ are independent across units and are distributed Type I Extreme Value, we get a standard conditional logit for $B(\cdot)$. However, other link functions---such as based on a nested logit or multinomial logit---could also be used.^[As discussed in the Appendix, the CBO health reform simulation model uses a nested logit formulation for $B(\cdot)$ in which individuals first select the type of coverage (e.g., employer, non-group, public, uninsured) and then, conditional on that choice, select among available plan types. An alternative approach, however, is to simply specify a reduced form equation that estimates or models $r_{ij} = P(y_{it} = j)$ directly. This type of approach is the basis for elasticity-based microsimulation models previously used by the CBO and used by other modelers [@AbrahamUsingmicrosimulationmodels2013; @GliedSimulationmodelinghealth2010; @GruberMicrosimulationestimateseffects2000; @HowCBOJCT2018; @SonierMedicaidwelcomemateffect2013].]

## Insurance Choice as a Markovian Process

However we specify the choice probabilities, the choice process at two discrete time periods ($t-1, t$) can be specified in terms of a Markov trace. Define the *ex ante occupancy vector* $\boldsymbol{\tilde p}$ summarizing the count or fraction of the population in each coverage category at time $t-1$ (i.e., at baseline). We also define a transition probability matrix $\boldsymbol{R_i} =  [r_{irs}]$. Cells in this $J \times J$ matrix are defined by transition probabilities based on conditional choice probabilities: $r_{irs} = P(y_{it} = s | y_{i,t-1}=r)$. 

With the ex ante occupancy vector and transition probability matrix defined, it is straightforward to obtain the *ex post occupancy vector* summarizing the fraction or number of people in each coverage category at time $t$: 

\begin{equation}
\label{eq:expost_1}
\boldsymbol{p} = \boldsymbol{\tilde{p}'R}
\end{equation}


## Simulating Counterfactual Policy Changes

A key takeaway is that the *ex ante* occupancy vector ($\boldsymbol{\tilde{p}}$) and the set of transition probabilities ($r_{rs}$) are sufficient to model hypothetical changes to coverage policy. To see this, note that the *ex ante* occupancy vector always remains fixed since it captures the coverage distribution at baseline, i.e., before any counterfactual reform. As such, it can be estimated using nationally-representative survey-based data (e.g., the Current Population Survey, the Medical Expenditure Panel Survey, etc.). 

Furthermore, we can express the transition probabilities in potential outcomes notation. That is, define $r_{irs}(0)$ as the probability of transition between coverage categories under the status quo, and $r_{irs}(1)$ as the probability of transition under a counterfactual reform scenario. We can similarly collect these transition probabilities in $J \times J$ transition probability matrices $\boldsymbol{R(0)}$ and $\boldsymbol{R(1)}$. 

The impact of reform on coverage is summarized as

\begin{equation}
\label{eq:takeup_potout}
  \boldsymbol{\theta} = \boldsymbol{p(1)} -  \boldsymbol{p(0)} 
  = \boldsymbol{\tilde{p}'R(1)} - \boldsymbol{\tilde{p}'R(0)}
\end{equation}

With this simple modeling framework in hand, we can efficiently estimate counterfactual changes in the distribution of health insurance; all that is required is estimates of $\boldsymbol{\tilde{p}}$ and the transition probabilities. As will be shown below, these changes in coverage can be combined with additional literature- or theory-based parameters to summarize the welfare benefits and costs of take-up (or loss) of public insurance, private insurance, etc. These estimates, in turn, facilitate comparative welfare assessments based on the MVPF of alternative coverage expansion policies.

# Methods

## Simulating Expansion of In-Kind Benefits 

- Baseline occupancy vector 



Our application of the modeling framework summarized by Equation \ref{eq:takeup_potout} curates and derives estimates of $\boldsymbol{R(1)}$ and $\boldsymbol{R(0)}$ from the applied literature. Additionally, we estimate the baseline coverage occupancy vector under the status quo $\boldsymbol{\tilde{p}}$ based on the American Community Survey (ACS). The key inputs and ingredients for the modeling application are summarized in Table TK. For example, Graves et al. (2020) outline estimation and inference procedures for directly estimating $\boldsymbol{R(0)}$ and $\boldsymbol{R(1)}$ to evaluate changes in coverage transitions after expansion of Medicaid. These estimates are then combined with parameters capturing individuals' valuation of public insurance from @FinkelsteinValueMedicaidInterpreting2015 to model the welfare benefits and costs of further expansion of in-kind public health insurance programs.

In a similar vein, the willingness-to-pay (WTP) and cost curves estimated in @FinkelsteinSubsidizinghealthinsurance2017 are used to derive coverage take-up probabilities and parameters summarizing costs and individuals' valuation of enrollment in private insurance plans. In principle, however, coverage takeup probabilities and parameters summarizing welfare benefits and costs of different policy scenarios could also be modeled directly using a detailed microsimulation model.^[A standard assumption is that an exogenous policy change does not affect the unobserved disturbance term $\epsilon_{itj}$. Moreover, policy changes will affect utility/take-up through their impact on prices, quality, offers of employment-based insurance, etc. To model these changes via microsimulation, attributes of plans and individuals in the microdata are adjusted to reflect the modeled reform scenario. In a utility maximization model, for example, differences in predicted utility are used to derive new unit-level choice probabilities under the modeled reform scenario. In a reduced-form (elasticity-based) microsimulation model, price changes for each of the $J$ insurance options are simulated for units in the microdata. Elasticities and further adjustments (e.g., income effects) are then applied to derive new choice probabilities. These aggregated choice probabilities, along with attributes of individuals (e.g., health status) and policy (e.g., subsidy schedules) are the building blocks for other modeled outcome changes (e.g., cost of subsidies, premiums, etc.).]

## Scenarios

Our application will consider two reform scenarios: (1) further expansion of Medicaid to all individuals under 150% of the federal poverty line (FPL); vs. (2) a price-linked ($25/month) subsidy to facilitate the purchase of private insurance plans. 

To model these scenarios, we estimate the coverage distribution for individuals <150% FPL using the 20TK American Community Survey (ACS). These estimates serve as the basis for the ex ante occupancy vector ($\boldsymbol{\tilde{p}}$) for the modeled population. As noted above and in Table TK, additional literature-based parameters (and their uncertainty distributions) are used to construct estimates of $\boldsymbol{R(0)}$ and $\boldsymbol{R(1)}$. In principle, however, future research could expand the number of scenarios to model a Medicare-for-all proposal and/or reforms to private insurance markets (e.g., mandates and changes to risk-adjustment and risk-pooling policies) in the spirit of Geruso et al. 2019. 

## Model Calibration

- Bayesian calibration using incremental mixture importance sampling
- Method focuses on estimating the joint posterior distribuiton of model parameters given model targets. 
- This can be used to construct model predictions that include uncertainty. 

## Parameter Uncertainty

## Metamodel 



# Results

We begin by summarizing the key parameters and inputs into the policy simulation. These values are summarized in Figure \ref{table_params}. Specifically, we simulate two policy scenarios: (1) expansion of Medicaid to those <138% FPL and (2) expansion of coverage via a price-linked subsidy that fixes monthly premiums at $36. With few exceptions all baseline values are drawn from the point estimates in @finkelstein_value_2019 and @finkelstein_subsidizing_2019.^[We extrapolate somewhat from the estimates in @finkelstein_subsidizing_2019 by assuming that a similar demand curve exists <150% FPL as at 150%, as estimated in that study.In their study, @finkelstein_subsidizing_2019 estimate similar demand curves at 150% FPL and 200% FPL, indicating that demand does not differ materially across low-income groups.]

```{r,out.width = "100%",echo = FALSE, fig.cap = "\\label{table_params}Baseline Parameters for Policy Simulation"}
knitr::include_graphics(here::here("./figures/03_simulate-subsidy-baseline-parameters.png"))
```

Figure \ref{fig_fhs19rep} replicates the willingness to pay and cost curves in @finkelstein_value_2019. Specific replication points are plotted in solid dots, and the solid curves reflect the extrapolated points derived from a third-degree polynomial fit to the points. Notably, the figure also plots fitted curves based on 1,000 multivariate normal draws from the estimated coefficients and variance-covariance matrix in the underlying regression-discontinuity regressions. These curves demonstrate the degree to which estimation precision in the underlying RD regression contribute to uncertainty in the estimated WTP and cost curves. This uncertainty, along with uncertainty in other model parameters, will later feed through the probabilistic sensitivity analysis. 

```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_fhs19rep}Replication of Finkelstein, Hendren and Shepard (2019) Figure 13"}
knitr::include_graphics(here::here("./figures/01_FHS-replication-fig12.png"))
```

Based on the model parameters and their underlying uncertainty, we next estimate overall policy benefits and costs for the two coverage expansion scenarios. These points are plotted in Figure \ref{fig_cost_and_benefit}. Based on the baseline model parameters we estimate a MVPF of 


\newpage
```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_cost_and_benefit}Cost and Benefit Estimates"}
knitr::include_graphics(here::here("./figures/03_cost-and-benefit-estimates.png"))
```
<!-- ![Cost and Benefit Estimates (M=1,000)](../figures/03_cost-and-benefit-estimates.png) -->
\newpage


\newpage

```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_oneway90}One Way Sensitivity Analysis (MVPF Benchmark = 0.90)"}
knitr::include_graphics(here::here("./figures/03_one-way-sensitivity_gov-incidence_lambda90.png"))
```
<!-- ![One Way Sensitivity Analysis ($\lambda = 0.9$): Government Incidence of Uncompensated Care](../figures/03_one-way-sensitivity_gov-incidence_lambda90.png -->

**Notes:**
\newpage



```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_oneway90}One Way Sensitivity Analysis (MVPF Benchmark = 0.90)"}
knitr::include_graphics(here::here("./figures/03_two-sensitivity_value-medicaid_gov-incidence.png"))
```
<!-- ![One Way Sensitivity Analysis ($\lambda = 0.9$): Government Incidence of Uncompensated Care](../figures/03_one-way-sensitivity_gov-incidence_lambda90.png -->

**Notes:**

\newpage 

```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_oneway20}One Way Sensitivity Analysis (lambda = 0.20)"}
knitr::include_graphics(here::here("./figures/03_one-way-sensitivity_gov-incidence_lambda20.png"))
```

**Notes:**
\newpage
<!-- ![One Way Sensitivity Analysis ($\lambda = 0.9$): Government Incidence of Uncompensated Care](../figures/03_one-way-sensitivity_gov-incidence_lambda20.png) -->



```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_policy_acceptability_frontier}Policy Acceptablity Frontier"}
knitr::include_graphics(here::here("./figures/03_policy-acceptability-frontier.png"))
```

**Notes:**
\newpage
<!-- ![Policy Acceptability Frontier](../figures/03_policy-acceptability-frontier.png) -->

\newpage
```{r, out.width = "100%",echo = FALSE, fig.cap = "\\label{fig_evppi}Expected Value of Partial Perfect Information"}
knitr::include_graphics(here::here("./figures/03_evppi.png"))
```

**Notes:**
\newpage

<!-- <!-- ![Expected Value of Partial Perfect Information](../figures/03_evppi.png) --> -->

```{r, eval = FALSE, child="../R/03_simulate-subsidy-and-medicaid.Rmd"}
```


# References
\singlespacing
<div id="refs"></div>


\newpage 

# Appendix TK: Replication of Finkelstein, Hendren and Shepard (2019) {-}


