
```{r, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```

```{r}
suppressWarnings(suppressMessages(source(here::here("./R/manifest.R"))))
library(directlabels)
library(gt)
source(here("./R/functions_finkelstein-hendren-shepard-replication.R"))
conflict_prefer("set_names","rlang")
conflict_prefer("filter","dplyr")
```

```{r}
# # Import datasets
# 1% Collapsed 2011 Dataset used to replicate the primary RD regressions
df <- 
  haven::read_dta(here("./data/AER_data_files/Data/Collapsed_CommCare/model_analysis_data_by1pctFPL_matchFPL_scaledACS.dta")) %>% 
  mutate(inRD_sample = ifelse((pyear == 2011 & (income_grp >= 135 & income_grp <= 300) & region == ""), 1, 0)) %>%
  mutate(aboveYCut1 = ifelse(income_grp > 150, 1, 0), 
         aboveYCut2 = ifelse(income_grp > 200, 1, 0),
         aboveYCut3 = ifelse(income_grp > 250, 1, 0)) %>%
  mutate(fpl_X_aboveYCut1 = income_grp * aboveYCut1, 
         fpl_X_aboveYCut2 = income_grp * aboveYCut2,
         fpl_X_aboveYCut3 = income_grp * aboveYCut3) %>%
  mutate(fpl_relcut1 = (income_grp - 150), 
         fpl_relcut2 = (income_grp - 200), 
         fpl_relcut3 = (income_grp - 250)) %>%
  mutate(fpl_X_aboveCut1 = fpl_relcut1*aboveYCut1, 
         fpl_X_aboveCut2 = fpl_relcut2*aboveYCut2, 
         fpl_X_aboveCut3 = fpl_relcut3*aboveYCut3) %>%
  mutate(cost_ins_all = cost_ins, 
         cost_ins_H = cost_ins_Hplan, 
         cost_ins_L = cost_ins_Lplan) %>% 
  select(nTotEnr, nTotEnr_Hplan, nNewEnr_new, nNewEnr_new_Hplan, 
         nTotEnr_Lplan,share_Ins = share_Insured, share_H = share_NonCeltPlan, share_CeltiCare,
         aboveYCut1, aboveYCut2, aboveYCut3, income_grp, fpl_X_aboveYCut1, fpl_X_aboveYCut2, fpl_X_aboveYCut3, 
         cost_ins_all, cost_ins_H, cost_ins_L,
         everything()) 


```

```{r}
#########################
# Willingness to Pay RDs
#########################

# Fit Low- Generosity/Premium Plan RD
fit_all <- 
  df %>% 
  estimate_fsh_rd(yvar = "share_Ins")

# Fit High-Low Difference RD
fit_HL <- 
  df %>% 
  estimate_fsh_rd(yvar = "share_H")

#####################
# Cost RDs
#####################

fit_cost_all <- 
  df %>% 
  estimate_fsh_rd(yvar = "cost_ins_all")

fit_cost_L <- 
  df %>% 
  estimate_fsh_rd(yvar = "cost_ins_L")

fit_cost_H <- 
  df %>% 
  estimate_fsh_rd(yvar = "cost_ins_H")
```

```{r}
###############
# Estimate WTP
###############
est_wtp <- 
  get_WTP(coef = fit_all$coef,
             coef_H = fit_HL$coef)
#################
# Estimate Costs
#################
est_cost <- 
  get_cost(coef_cost =  fit_cost_H$coef,
          coef_D  = fit_HL$coef)
```

```{r}
# Extrapolated WTP
df_est_wtp <- 
  est_wtp %>% 
  select(fpl_group,demand_L_ext,demand_H_ext) %>% 
         unnest() %>% 
         select(fpl_group,s,price_L = price, price_H = price1) %>% 
         gather(type,price,-fpl_group,-s) %>% 
         mutate(type = gsub("price_","",type)) %>% 
  select(fpl_group , type, s , wtp = price)

# Extrapolated Costs
df_est_cost <- 
  est_cost %>% 
  pluck("cost_H_ext") %>% 
  set_names(c("fpl150","fpl200","fpl250")) %>% 
  bind_rows(.id  = "fpl_group") %>% 
  select(fpl_group, s = share, cost,avg_cost = premium) %>% 
  mutate(type = "H") %>% 
  na.omit() %>% 
  select(fpl_group, type, s, cost, avg_cost)

df_est <- 
  df_est_wtp %>% 
  left_join(df_est_cost,c("fpl_group","type","s"))

df_est %>% write_rds(here("./output/demand-estimates/01_wtp-and-cost-extrapolated-points.rds"))
```

```{r}
df_est_wtp_points <- 
  est_wtp %>% 
  select(fpl_group,demand_L_est) %>% 
         unnest() %>% 
         select(fpl_group,s = demand_L, price = price) %>% 
  mutate(type = "L") %>% 
  bind_rows(
    est_wtp %>% 
    select(fpl_group,demand_H_est) %>% 
         unnest() %>% 
         select(fpl_group, s= s, price = price) %>% 
      mutate(type = "H")
  ) %>% 
  select(fpl_group , type, s , wtp = price)

df_est_cost_points <-
  est_cost %>% 
    pluck("cost_H_est") %>% 
    set_names(c("fpl150","fpl200","fpl250")) %>% 
    bind_rows(.id  = "fpl_group") %>% 
    mutate(type = "H") %>% 
    select(fpl_group, type, s = share, avg_cost = premium)  %>% 
  bind_rows(
  est_cost %>% 
    pluck("cost_H_est2") %>% 
    set_names(c("fpl150","fpl200","fpl250")) %>% 
    bind_rows(.id  = "fpl_group") %>% 
    mutate(type = "H") %>% 
    select(fpl_group, type, s = share, cost) 
  )

df_est_points <- 
  df_est_wtp_points %>% 
  bind_rows(df_est_cost_points)

df_est_points %>% write_rds(here("./output/demand-estimates/01_wtp-and-cost-points.rds"))
```

```{r}
p <- df_est %>% 
  filter(fpl_group=="fpl150") %>% 
  gather(key,value,-fpl_group,-type,-s) %>% 
  filter(type=="H") %>% 
  ggplot(aes(x = s , y = value,colour = key)) + 
  facet_wrap(fpl_group~type) + 
  #geom_line(lty=3) + 
  ggthemes::theme_clean() +  
  ylim(c(0,500)) + 
  xlim(c(0,1)) +
  geom_point(data = 
               df_est_points %>% 
               filter(fpl_group=="fpl150") %>% 
               gather(key,value,-fpl_group,-type,-s) %>% 
               filter(type=="H") 
               ) +
  geom_line() +
  geom_line(data = 
               df_est_points %>% 
              filter(fpl_group=="fpl150") %>% 
               gather(key,value,-fpl_group,-type,-s) %>% 
               filter(type=="H") 
               ) + 
  ylab("$ per month") + 
  xlab("Fraction in plan (=s)") + 
  ggtitle("Replication of Finkelstein, Hendren and Shepard (2019)\nFigure 12")
#direct.label(p)
```


```{r, eval = TRUE}
# WTP and Costs (Primary Estimates)
df_wtp_and_costs <- 
  fit_wtp_and_costs(
    coef_wtp_l = fit_all$coef,
    coef_wtp_hl = fit_HL$coef,
    coef_cost_h = fit_cost_H$coef, 
    coef_cost_demand = fit_HL$coef
  ) %>% 
  mutate(iteration = "0")

df_wtp_and_costs %>% 
   write_rds(here("./output/demand-estimates/01_wtp-and-cost.rds"))

df_wtp_and_costs_data <- 
  fit_wtp_and_costs(
    coef_wtp_l = fit_all$coef,
    coef_wtp_hl = fit_HL$coef,
    coef_cost_h = fit_cost_H$coef, 
    coef_cost_demand = fit_HL$coef,
    output_type = "data"
  ) %>% 
  mutate(iteration = "0")


fit_uncertainty  = FALSE

if (fit_uncertainty) {
  df_uncert_wtp_and_costs <-
    1:1000 %>%
    map(~(
      fit_wtp_and_costs(
          coef_wtp_l = MASS::mvrnorm(n = 1, mu = coefficients(fit_all$fit), Sigma = fit_all$vcov),
          coef_wtp_hl = MASS::mvrnorm(n = 1, mu = coefficients(fit_HL$fit), Sigma = fit_HL$vcov),
          coef_cost_h = MASS::mvrnorm(n = 1, mu = coefficients(fit_cost_H$fit), Sigma =fit_cost_H$vcov),
          coef_cost_demand = MASS::mvrnorm(n = 1, mu = coefficients(fit_HL$fit), Sigma = fit_HL$vcov))
    )) %>%
      bind_rows(.id = "iteration")
  
  df_uncert_wtp_and_costs %>% 
   write_rds(here("./output/demand-estimates/01_wtp-and-cost_uncertainty.rds"))
  
   df_uncert_wtp_and_costs_data <-
    1:1000 %>%
    map(~(
      fit_wtp_and_costs(
          coef_wtp_l = MASS::mvrnorm(n = 1, mu = coefficients(fit_all$fit), Sigma = fit_all$vcov),
          coef_wtp_hl = MASS::mvrnorm(n = 1, mu = coefficients(fit_HL$fit), Sigma = fit_HL$vcov),
          coef_cost_h = MASS::mvrnorm(n = 1, mu = coefficients(fit_cost_H$fit), Sigma =fit_cost_H$vcov),
          coef_cost_demand = MASS::mvrnorm(n = 1, mu = coefficients(fit_HL$fit), Sigma = fit_HL$vcov),
      output_type = "data")
    )) %>%
      bind_rows(.id = "iteration")
   
  df_uncert_wtp_and_costs_data %>% 
   write_rds(here("./output/demand-estimates/02_wtp-and-cost_uncertainty-data.rds"))
  
   
}

df_uncert_wtp_and_costs <- read_rds(here("./output/demand-estimates/01_wtp-and-cost_uncertainty.rds"))
df_uncert_wtp_and_costs_data <- read_rds(here("./output/demand-estimates/02_wtp-and-cost_uncertainty-data.rds"))
```

```{r validation}
df_validation <- 
  data.frame(fpl = rep(c(150,150,150,150),1), type = "H", prem = c(0,36,105,246))  %>% 
    inner_join(df_wtp_and_costs,c("fpl","type")) %>% 
    mutate(estimate = intercept + wtp * prem + i_wtp_2 * prem^2 + i_wtp_3 * prem ^3) %>% 
    mutate(fpl_group = paste0("fpl",fpl)) %>% 
    select(fpl_group,type,wtp = prem, outcome,estimate,iteration) %>% 
    spread(outcome,estimate) %>% 
    gather(key,value,-fpl_group,-type,-s,-iteration)

df_validation_uncert <- 
  data.frame(fpl = rep(c(150,150,150,150),1), type = "H", prem = c(0,36,105,246)) %>% 
    inner_join(df_uncert_wtp_and_costs,c("fpl","type")) %>% 
    mutate(estimate = intercept + wtp * prem + i_wtp_2 * prem^2 + i_wtp_3 * prem ^3) %>% 
    mutate(fpl_group = paste0("fpl",fpl)) %>% 
    select(iteration,fpl_group,type,wtp = prem, outcome,estimate) %>% 
    spread(outcome,estimate) %>% 
    gather(key,value,-fpl_group,-type,-s,-iteration)

p2 <- 
  p + 
  geom_point(data = df_validation_uncert %>% filter(fpl_group=="fpl150"),alpha = 0.05)  +
    geom_point(data = df_validation %>% filter(fpl_group=="fpl150"),colour = "black",pch = 3)  + 
  theme(text = element_text(family = "Gill Sans"),
        plot.title = element_text(family = "Gill Sans", face = "plain"),
        plot.background = element_blank()) + 
  geom_dl(aes(label = key), method = list("last.polygons",cex =0.9,fontfamily = "Gill Sans"))  + 
  scale_x_continuous(limits = c(0,1.3),breaks = seq(0,1,0.2)) + 
  theme(legend.position="none")

#p2
```
\newpage 
```{r}
validation_summ <- 
  df_validation_uncert %>% 
  tbl_df() %>% 
  spread(key,value) %>% 
  group_by(wtp) %>% 
  summarise_at(vars(s,cost),list(sd = ~sd(pmax(0,.))))
df_validation %>% 
  spread(key,value) %>% 
  left_join(validation_summ,"wtp") %>% 
  select(premium = wtp, takeup= s, takeup_se = s_sd, cost = cost, cost_se = cost_sd) %>% 
  gt() %>% 
  tab_header(
    title = "Replication of Finkelstein, Hendren and Shepard (2019) Table 2"
  ) %>% 
  fmt_number(
    columns = vars(takeup),
    decimals = 3,
    use_seps = FALSE
  ) %>% 
    fmt_number(
    columns = vars(takeup_se),
    decimals = 2,
    use_seps = FALSE
  ) %>% 
  fmt_currency(
    columns = vars(cost,cost_se),
    decimals = 0,
    use_seps = FALSE
  )
```



```{r, eval = TRUE}
# get_mvpf_subsidy(wtp = .x$wtp, cost = .x$cost, )

fn_uncomp <- function(cost, uninsured_oop_share , phi ) {
  # x is the share of the uninsured’s total health care costs that they pay out of pocket 
  # φ denotes the percentage increase in costs that result from insurance coverage (moral hazard)
  (1 - uninsured_oop_share) * (cost / (1 + phi))
}

df_mvpf <-
  df_wtp_and_costs_data %>% 
  bind_rows(df_uncert_wtp_and_costs_data) %>% 
  mutate(mvpf_orig = 1 / (1 + ((cost - wtp) / (-s * ((wtp-lag(wtp))/.01))))) %>% 
  filter(type == "H" & fpl_group =="fpl150") %>% 
  na.omit()

get_mvpf_subsidy <- function(df, cost, wtp, s, gov_incidence = 0, uninsured_oop_share = 0.2, phi = 0.25, eta = 0) {
  df %>% 
    #mutate(numerator = 1) %>% 
    #mutate(denominator = (1 + (({{cost}} - {{wtp}}) / (-{{s}} * (({{wtp}}-lag({{wtp}}))/.01))))) %>% 
    mutate(gov_incidence = gov_incidence, 
          x_oop_uninsured = uninsured_oop_share, 
          phi_moral_hazard = phi, 
          eta_incidence_lowinc = eta) %>% 
    mutate(uncomp = fn_uncomp(cost = cost,uninsured_oop_share = uninsured_oop_share, phi = phi)) %>% 
    mutate(
      mvpf_num = {{s}} + 
        eta  * 
          (
            uncomp / 
              (-1 * (( {{wtp}} - lag({{wtp}})) / 0.01))
          )
    ) %>% 
    mutate(
      mvpf_denom =  {{s}} + 
        ((pmax(0,{{cost}} - gov_incidence * uncomp - {{wtp}})) / 
           (-1 * (({{wtp}} - lag({{wtp}}))/.01)))
    ) %>% 
    mutate(mvpf = mvpf_num / mvpf_denom)

}

mvpf_no_uncomp <-   
  df_wtp_and_costs_data %>% 
  bind_rows(df_uncert_wtp_and_costs_data) %>% 
  get_mvpf_subsidy(cost = cost, wtp = wtp , s = s, uninsured_oop_share = 1, phi = 0, gov_incidence = 0, eta = 0) %>% 
  filter(type=="H" & fpl_group=="fpl150" & s %in% c(0.3, 0.9))   %>% 
  mutate(scenario = "No Uncompensated\nCare")
  
# Incidence on Government
mvpf_govt<-   
  df_wtp_and_costs_data %>% 
  bind_rows(df_uncert_wtp_and_costs_data) %>% 
  get_mvpf_subsidy(cost = cost, wtp = wtp , s = s, uninsured_oop_share = 0.2, phi = 0.25, gov_incidence = 1, eta = 0) %>% 
  filter(type=="H" & fpl_group=="fpl150" & s %in% c(0.3, 0.9))  %>% 
  mutate(scenario = "Incidence on\nGovernment")
  
# Incidence on Low-Income Uninsured
mvpf_lowinc1 <-   
  df_wtp_and_costs_data %>% 
  bind_rows(df_uncert_wtp_and_costs_data) %>% 
  get_mvpf_subsidy(cost = cost, wtp = wtp , s = s, uninsured_oop_share = 0.2, phi = 0.25, gov_incidence = 0, eta = 1) %>% 
  filter(type=="H" & fpl_group=="fpl150" & s %in% c(0.3, 0.9))   %>% 
  mutate(scenario = "Incidence on Low\nIncome Uninsured")

# Incidence on Affulent
mvpf_lowinc0_5 <-   
  df_wtp_and_costs_data %>% 
  bind_rows(df_uncert_wtp_and_costs_data) %>% 
  get_mvpf_subsidy(cost = cost, wtp = wtp , s = s, uninsured_oop_share = 0.2, phi = 0.25, gov_incidence = 0, eta = 0.5) %>% 
  filter(type=="H" & fpl_group=="fpl150" & s %in% c(0.3, 0.9))   %>% 
  mutate(scenario = "Incidence on\nAffulent")

df_mvpf_replication <- 
  mvpf_no_uncomp %>% 
    bind_rows(mvpf_govt) %>% 
    bind_rows(mvpf_lowinc1) %>% 
    bind_rows(mvpf_lowinc0_5) %>% 
  mutate(s = paste0(s*100,"% Insured")) %>% 
  mutate(scenario = factor(scenario, levels = levels(factor(scenario))[c(1,3,2,4)]))
                           
sdf_mvpf_replication <- 
  mvpf_no_uncomp %>% 
    bind_rows(mvpf_govt) %>% 
    bind_rows(mvpf_lowinc1) %>% 
    bind_rows(mvpf_lowinc0_5) 

# df_mvpf_replication %>% 
#   ggplot( aes(x = scenario,fill = s)) + 
#   scale_x_discrete(limits = rev(levels(df_mvpf_replication$scenario ))) + 
#   geom_col(data = df_mvpf_replication  %>% filter(iteration == 0), aes(y = mvpf),position = "dodge") + 
#   ggthemes::theme_clean() + 
#   geom_hline(aes(yintercept = 0.9),lty=2) + 
#   geom_hline(aes(yintercept = 0.2),lty=2) +
#   scale_y_continuous(limits = c(0,1.5), breaks = seq(0,1.5,0.25)) + 
#   geom_text(data = df_mvpf_replication  %>% filter(iteration == 0),
#             aes(y = mvpf+.08,label = round(mvpf,2)), position = position_dodge(0.9), family = "Gill Sans") + 
#   theme(legend.position = "bottom") + 
#   ggtitle("Replication of Finkelstein, Hendren and Shepard (2017)\nFigure 14") +
#   ylab("MVPF")  + xlab("") + 
#   theme(text = element_text(family = "Gill Sans"),
#         plot.title = element_text(family = "Gill Sans", face = "plain"),
#         plot.background = element_blank(), legend.title = element_blank())
```

```{r ,eval = FALSE}
get_params <-  function(x,params) {
  map2(params,x,~(
    .x(.y)
  ))
} 
qfixed <- function(x, value) value


get_takeup_coef <- function(df  = df_wtp_and_costs,params) {
  sampled <- sample(df$iteration,1)
  tmp <- 
    df  %>% 
    filter(iteration == sampled & fpl == params$pop_fpl & type == params$plan_type & outcome=="s") %>% 
    select(-type,-outcome,-fpl,-iteration) 
  
  tmp_coef <- 
    tmp %>% 
    gather(coef,value) %>% 
    mutate(value = as.numeric(paste0(value))) %>% 
    pull(value) 
  names(tmp_coef) <- names(tmp)
  
  return(tmp_coef)
  
}

get_cost_coef <- function(df = df_wtp_and_costs, params) {
  sampled <- sample(df$iteration,1)
  tmp <- 
    df %>% 
    filter(iteration == sampled & fpl == params$pop_fpl & type == params$plan_type & outcome=="cost") %>% 
    select(-type,-outcome,-fpl,-iteration) 
    
  tmp_coef <- 
    tmp %>% 
    gather(coef,value) %>% 
    mutate(value = as.numeric(paste0(value))) %>% 
    pull(value) 
  names(tmp_coef) <- names(tmp)
  
  return(tmp_coef)

}

get_takeup <- function(params, premium ) {
  
  if (is.null(premium)) prem = params$plan_premium else prem = premium
  
  tmp_out <- 
    data.frame(fpl = params$pop_fpl, 
             type = params$plan_type, 
             prem = prem) %>% 
    mutate(estimate = params$takeup_coef['intercept'] + params$takeup_coef['wtp'] * prem + params$takeup_coef['i_wtp_2'] * prem^2 + params$takeup_coef['i_wtp_3'] * prem ^3) %>% 
    pull(estimate) %>% unname()
  
  return(pmax(0,pmin(1,tmp_out)))
  
}

get_cost <- function(params, premium ) {
  
  if (is.null(premium)) prem = params$plan_premium else prem = premium
  
  tmp_out <- 
    data.frame(fpl = params$pop_fpl, 
               type = params$plan_type, 
               prem = prem) %>% 
    mutate(estimate = params$cost_coef['intercept'] + params$cost_coef['wtp'] * prem + params$cost_coef['i_wtp_2'] * prem^2 + params$cost_coef['i_wtp_3'] * prem ^3) %>% 
    pull(estimate) %>% unname()
  
  return(pmax(0,tmp_out))
  
}

params <- 
  list(
    pop_fpl = 150,
    plan_type = "H",
    gov_incidence = 0.5, 
    uninsured_oop_share = 0.2, 
    phi = 0.25, 
    eta = 0.75, 
    plan_premium = 36,
    baseline_premium = 50
  )
# Parameters that are functions of other parameters
params$takeup_coef = get_takeup_coef(params = params)
params$cost_coef = get_cost_coef(params = params)

simulate_subsidy <- function(params) {
  takeup_baseline <- get_takeup(params, premium = params$baseline_premium)
  takeup_baseline_deriv = 1/(get_takeup(params, premium = params$baseline_premium+1) - get_takeup(params, premium = params$baseline_premium))
  takeup_reformed <- get_takeup(params, premium = params$plan_premium)
  takeup_reformed_deriv = 1/(get_takeup(params, premium = params$plan_premium+1) - get_takeup(params, premium =  params$plan_premium))
  cost_baseline <- get_cost(params, premium = params$baseline_premium)
  cost_reformed <- get_cost(params, premium = params$plan_premium)
  uncomp = fn_uncomp(cost = cost_reformed, uninsured_oop_share = params$uninsured_oop_share, phi = params$phi)
  
  mvpf_num = takeup_reformed + 
    params$eta * 
    (
      pmax(0,uncomp) / 
        (-1 * takeup_reformed_deriv)
    ) 
  mvpf_denom = takeup_reformed + 
    ((pmax(0,cost_reformed - params$gov_incidence * uncomp - params$plan_premium)) / 
       (-1 * takeup_reformed_deriv)) 
  mvpf = mvpf_num / mvpf_denom
  
  
  mvpf_num_baseline = takeup_baseline + 
    params$eta * 
    (
      pmax(0,uncomp) / 
        (-1 * takeup_baseline_deriv)
    ) 
  mvpf_denom_baseline = takeup_baseline + 
    ((pmax(0,cost_baseline - params$gov_incidence * uncomp - params$plan_premium)) / 
       (-1 * takeup_reformed_deriv)) 
  mvpf_baseline = mvpf_num_baseline / mvpf_denom_baseline
  
  
  
  output <- list(takeup_baseline = takeup_baseline,
                 takeup_baseline_deriv = takeup_baseline_deriv,
                 takeup_reformed = takeup_reformed, 
                 takeup_reformed_deriv = takeup_reformed_deriv, 
                 cost_baseline = cost_baseline, 
                 cost_reformed = cost_reformed,
                 uncomp = uncomp, 
                 mvpf_num = mvpf_num,
                 mvpf_denom = mvpf_denom, 
                 mvpf = mvpf,
                 mvpf_num_baseline = mvpf_num_baseline,
                 mvpf_denom_baseline = mvpf_denom_baseline, 
                 mvpf_baseline = mvpf_baseline
                 )
  return(output)
}

#simulate_subsidy(params)

### Probabilistic Sensitivity Analysis

params_psa <- params
params_psa$gov_incidence = function(x) qunif(p = x, min = 0, max = 1) # Incidence on Government
params_psa$eta = function(x) qunif(p = x, min =0.5, max = 1) # Social Welfare Weight
params_psa$phi = function(x) qnorm(p = x, mean = 0.25,sd = 0.05) # Moral Hazard Effects of Insurance
params_psa$uninsured_oop_share =  function(x) pmax(0,pmin(1,qnorm(p = x, mean = 0.2,sd = 0.1))) # Share OOP for Uninsured

# Only use Halton draws for PSA parameters that aren't fixed.
which_are_fns <- params_psa %>% map_lgl(~is.function(.x)) 

# Raw halton draw
draws <- randtoolbox::halton(n = 1e3, dim = length(params_psa[which_are_fns])) %>% as.matrix()

set.seed(23)

halton_draws <- 
  draws %>% 
  data.frame() %>% 
  magrittr::set_names(names(params_psa[which_are_fns]))  %>% 
  pmap(list) %>% 
  map(~(get_params(.x,params=params_psa[which_are_fns]))) %>% 
  map(~(data.frame(.x))) %>% bind_rows() %>% 
  pmap(list)

params_halton <- 
  halton_draws %>% 
  map(~(c(.x,params_psa[!which_are_fns]))) 

params_halton_transpose <- 
  params_halton %>% 
  transpose()

params_halton_transpose$takeup_coef <- 
  params_halton %>% 
  map( ~get_takeup_coef(df=df_uncert_wtp_and_costs, params = .x))  
  #map( ~get_takeup_coef(df=df_wtp_and_costs, params = .x))  

params_halton_transpose$cost_coef <- 
  params_halton %>% 
  map( ~get_cost_coef(df=df_uncert_wtp_and_costs, params = .x))  
  #map( ~get_cost_coef(df=df_wtp_and_costs, params = .x))  

params_halton_final <- 
  params_halton_transpose %>% 
  transpose()

df_psa <- 
  params_halton_final %>% 
  map(~(simulate_subsidy(.x))) %>% 
  map(~(data.frame(.x))) %>% 
  bind_rows(.id = "iteration")

df_est <- 
  simulate_subsidy(params) %>% 
  data.frame() %>% 
  mutate(iteration = "0") %>% 
  mutate(label_reformed = paste0(" Modeled Estimate:\n $36/mo premium\n (MVPF = ",round(mvpf,3),")")) %>% 
  mutate(label_baseline = paste0(" Modeled Estimate:\n $246/mo premium\n (MVPF = ",round(mvpf_baseline,3),")"))


# df_psa %>% 
#   ggplot(aes(x = mvpf_num, y = mvpf_denom)) +
#   geom_point(alpha = 0.1) + 
#   ggthemes::theme_clean() + 
#   scale_y_continuous(limits = c(0,4), breaks = seq(0,4,.25), name = "Cost") + 
#   scale_x_continuous(limits = c(0,4), breaks = seq(0,4,0.25), name = "Benefit") +
#   geom_abline(intercept = 0, slope = 1/.9, lty = 2)   +
#   #eom_rug(sides = "tr", length = unit(0.01,"npc"))  + 
#   geom_point(data = df_est, colour = "black", pch = 19) + 
#   geom_text(data = df_est, colour = "black",aes(label = label_reformed),hjust =0,vjust=1,size = 3,fontface="bold") +
#   geom_point(aes(x = mvpf_num_baseline, y = mvpf_denom_baseline), colour = "blue", alpha = 0.1) + 
#   geom_point(data = df_est, colour = "blue",aes(x = mvpf_num_baseline, y = mvpf_denom_baseline), pch = 19) + 
#   geom_text(data = df_est, colour = "blue",aes(x = mvpf_num_baseline, y = mvpf_denom_baseline,label = label_baseline),hjust =0,vjust=0,size = 3,fontface = "bold") + 
#   annotate("text",x = 3,y = 2.75,label = "Policies with\nFavorable MVPF",size = 3,hjust =0,vjust =0) +
#   annotate("text",y = 3.5,x = 2,label = "Policies with\nUnfavorable MVPF",size = 3,hjust = 0, vjust =0)  +
#   ggtitle("Modeled MVPF Estimates for a $246/mo and $36/mo\nSubsidized Premium With\nProbabilistic Sensitivity Analysis Results")

```

```{r, eval = FALSE}
TwoWaySA<-function(indata,outcome="NHB",parm1,parm2,range1,range2,lambda){
  
  # Get Outcome
  lhs <- indata %>% select(psa_id,contains("dQALY"),contains("dCOST")) %>%
    mutate(psa_id=row_number()) %>% 
    reshape2::melt(id.vars='psa_id') %>%
    tidyr::separate(variable,c("outcome","strategy"),"_") %>%
    reshape2::dcast(psa_id+strategy~outcome) %>%
    mutate(NHB = dQALY-dCOST/lambda ,
           NMB = dQALY*lambda - dCOST)
  

  # Get Parameters
  rhs <- indata %>% select(-contains("dQALY"),-contains("dCOST"),
                           -contains("NMB"),-contains("NHB"),-psa_id)
  
  # Map to existing code inputs
  Strategies <- unique(lhs$strategy)
  Parms <- rhs %>% tbl_df()  %>% data.frame() 
  cat(outcome)
  lhs$Y <- lhs[,outcome]
  Outcomes <- lhs %>% select(strategy,psa_id,Y) %>%
    reshape2::dcast(psa_id~strategy,value.var="Y") %>%
    select(-psa_id)
  

  #Extract parameter column number in Parms matrix
  x1<-which(colnames(Parms)==parm1)
  x2<-which(colnames(Parms)==parm2)
  dep<-length(Strategies) #Number of dependent variables, i.e., strategies
  indep<-ncol(Parms) #Number of independent variables, i.e., parameters
  
  Sim <- data.frame(Outcomes,Parms)
  
  if (ncol(Parms)==2) {
    Parms$constant = 1
    Sim$constant = 1
  }
  
  #Determine range of of the parameer to be plotted
  if (!missing("range1")&!missing("range2")){ #If user defines a range
    vector1<-seq(from=range1[1],to=range1[2],length.out=301)
    vector2<-seq(from=range2[1],to=range2[2],length.out=301)
  } else if (!missing("range1")&missing("range2")){ #Default range given by the domanin of the parameter's sample
    #vector to define 400 samples between the 2.5th and 97.5th percentiles
    vector1<-seq(from=range1[1],to=range1[2],length.out=301)
    y2 = seq(2.5,97.5,length.out=301)
    j2 = round(y2*(length(Parms[,x2])/100)) #indexing vector;j=round(y*n/100) where n is the size of vector of interest
    vector2<-sort(Parms[j2,x2])   
  } else if (missing("range1")&!missing("range2")){ #Default range given by the domanin of the parameter's sample
    #vector to define 400 samples between the 2.5th and 97.5th percentiles
    vector2<-seq(from=range2[1],to=range2[2],length.out=301)
    y1 = seq(2.5,97.5,length.out=301)
    j1 = round(y1*(length(Parms[,x1])/100)) #indexing vector;j=round(y*n/100) where n is the size of vector of interest
    vector1<-sort(Parms[j1,x1])   
  } else{
    y1 = seq(2.5,97.5,length.out=301)
    y2 = seq(2.5,97.5,length.out=301)
    j1 = round(y1*(length(Parms[,x1])/100)) #indexing vector;j=round(y*n/100) where n is the size of vector of interest
    j2 = round(y2*(length(Parms[,x2])/100))
    vector1<-sort(Parms[j1,x1])
    vector2<-sort(Parms[j2,x2])
  }
  #Generate a formula by pasting column names for both dependent and independent variables
  f <- as.formula(paste('cbind(',paste(colnames(Sim)[1:dep],collapse=','),
                        ') ~ (','poly(',parm1,',8)+','poly(',parm2,',8)+' ,
                        paste(colnames(Parms)[c(-x1,-x2)], collapse='+'),')'))
  #Run Multiple Multivariate Regression (MMR) Metamodel
  Tway.mlm = lm(f,data=Sim)
  
  TWSA <- expand.grid(parm1=vector1,parm2=vector2)
  
  #Generate matrix to use for prediction
  Sim.fit<-matrix(rep(colMeans(Parms)),nrow=nrow(TWSA),ncol=ncol(Parms), byrow=T)
  Sim.fit[,x1]<-TWSA[,1]
  Sim.fit[,x2]<-TWSA[,2]
  Sim.fit<-data.frame(Sim.fit) #Transform to data frame, the format required for predict
  colnames(Sim.fit)<-colnames(Parms) #Name data frame's columns with parameters' names
  
  #Predict Outcomes using MMMR Metamodel fit
  Sim.TW = data.frame(predict(Tway.mlm, newdata = Sim.fit))
  #Find optimal strategy in terms of maximum Outcome
  Optimal <- max.col(Sim.TW)
  #Get Outcome of Optimal strategy
  OptimalOut<-apply(Sim.TW,1,max)
  
  plotdata = Sim.fit #Append parameter's dataframe to predicted outcomes dataframe
  
  #A simple trick to define my variables in my functions environment
  plotdata$parm1<-plotdata[,parm1];
  plotdata$parm2<-plotdata[,parm2];
  
  plotdata$Strategy<-factor(Optimal,labels=Strategies[as.numeric(names(table(Optimal)))])
  plotdata$value<-OptimalOut
  
  txtsize<-12
  p <- ggplot(plotdata, aes(x=parm1,y=parm2))+
    geom_tile(aes(fill=Strategy)) +
    theme_bw() +
    #ggtitle(expression(atop("Two-way sensitivity analysis",
    #                        atop("Net Health Benefit")))) +
    scale_fill_discrete("Strategy: ", l=50)+
    xlab(parm1)+
    ylab(parm2)+
    theme(legend.position="bottom",legend.title=element_text(size = txtsize),
          legend.key = element_rect(colour = "black"),
          legend.text = element_text(size = txtsize),
          title = element_text(face="bold", size=15),
          axis.title.x = element_text(face="bold", size=txtsize),
          axis.title.y = element_text(face="bold", size=txtsize),
          axis.text.y = element_text(size=txtsize),
          axis.text.x = element_text(size=txtsize))+
    scale_fill_grey(start = 0, end = 1)
  return(p)
}


OneWaySA<-function(indata,outcome="NHB",lambda,parm,range){
  
  # Get Outcome
  lhs <- indata %>% select(psa_id,contains("dQALY"),contains("dCOST")) %>% 
    mutate(psa_id = row_number()) %>% 
    reshape2::melt(id.vars='psa_id') %>% 
    tidyr::separate(variable,c("outcome","strategy"),"_") %>% 
    reshape2::dcast(psa_id+strategy~outcome) %>% 
    mutate(NHB = dQALY-dCOST/lambda , 
           NMB = dQALY*lambda - dCOST) 
  
  # Get Parameters
  rhs <- indata %>% select(-contains("dQALY"),-contains("dCOST"),
                               -contains("NMB"),-contains("NHB"),-psa_id)
  
  # Map to existing code inputs
  Strategies <- unique(lhs$strategy)
  Parms <- rhs %>% tbl_df() %>% data.frame()
  lhs$Y <- lhs[,outcome]
  Outcomes <- lhs %>% select(strategy,psa_id,Y) %>% 
    reshape2::dcast(psa_id~strategy,value.var="Y") %>% 
    select(-psa_id)
  
  
  #Extract parameter column number in Parms matrix
  x<-which(colnames(Parms)==parm)
  dep<-length(Strategies) #Number of dependent variables, i.e., strategies outcomes
  indep<-ncol(Parms) #Number of independent variables, i.e., parameters
  Sim <- data.frame(Outcomes,Parms)
  #Determine range of of the parameer to be plotted
  if (!missing("range")){ #If user defines a range
    vector<-seq(range[1],range[2],length.out=400)
  }
  else{ #Default range given by the domanin of the parameter's sample
    #vector to define 400 samples between the 2.5th and 97.5th percentiles
    y = seq(2.5,97.5,length=400) 
    j = round(y*(length(Parms[,x])/100)) #indexing vector;j=round(y*n/100) where n is the size of vector of interest
    vector<-sort(as.data.frame(Parms)[j,x])    
  }
  
  #Generate a formula by pasting column names for both dependent and independent variables. Imposes a 1 level interaction
  f <- as.formula(paste('cbind(',paste(colnames(Sim)[1:dep],collapse=','), ') ~ (','poly(',parm,',2)+' ,paste(colnames(Parms)[-x], collapse='+'),')'))
  #Run Multiple Multivariate Regression (MMR) Metamodel
  Oway.mlm = lm(f,data=Sim)
  
  #Generate matrix to use for prediction 
  Sim.fit<-matrix(rep(colMeans(Parms)),nrow=length(vector),ncol=ncol(Parms), byrow=T)
  Sim.fit[,x]<-vector
  Sim.fit<-data.frame(Sim.fit) #Transform to data frame, the format required for predict
  colnames(Sim.fit)<-colnames(Parms) #Name data frame's columns with parameters' names
  
  #Predict Outcomes using MMMR Metamodel fit
  plotdata = data.frame(predict(Oway.mlm, newdata = Sim.fit))
  colnames(plotdata) <- Strategies #Name the predicted outcomes columns with strategies names
  
  #Reshape dataframe for ggplot
  plotdata = stack(plotdata, select=Strategies) #
  plotdata = cbind(Sim.fit, plotdata) #Append parameter's dataframe to predicted outcomes dataframe
  
  #A simple trick to define my variables in my functions environment
  plotdata$parm<-plotdata[,parm];
  library(directlabels)
  txtsize<-12 #Text size for the graphs
#   ggplot(data = plotdata, aes(x = parm, y = values, lty = ind)) +
#     geom_line() +
#     ggtitle("One-way sensitivity analysis \n Net Health Benefit") +
#     xlab(parm) +
#     ylab("E[NHB]") +
#     scale_colour_hue("Strategy", l=50) +
#     #scale_x_continuous(breaks=number_ticks(6)) + #Adjust for number of ticks in x axis
#     #scale_y_continuous(breaks=number_ticks(6)) +
#     theme_bw() +
#     theme(legend.position="bottom",legend.title=element_text(size = txtsize),
#           legend.key = element_rect(colour = "black"),
#           legend.text = element_text(size = txtsize),
#           title = element_text(face="bold", size=15),
#           axis.title.x = element_text(face="bold", size=txtsize),
#           axis.title.y = element_text(face="bold", size=txtsize),
#           axis.text.y = element_text(size=txtsize),
#           axis.text.x = element_text(size=txtsize))+
#     geom_dl(aes(label = ind), method = list(dl.combine("last.bumpup"), cex = 0.8))
# }


```


```{r, eval = FALSE}
df_psa_params <- 
  params_halton_final %>% 
  map(~(data.frame(.x) %>% 
          rownames_to_column(var = "coef_name") %>% 
          gather(key,value,takeup_coef,cost_coef) %>% 
          unite(key,coef_name,key) %>% 
          spread(key,value))) %>% 
  bind_rows(.id = "iteration")

df_psa_full <- 
  df_psa %>% tbl_df() %>% 
    mutate(dQALY_HIGHSUBSIDY = mvpf_num, 
           dCOST_HIGHSUBSIDY = mvpf_denom,
           dQALY_LOWSUBSIDY = mvpf_num_baseline,
           dCOST_LOWSUBSIDY = mvpf_denom_baseline) %>% 
    select(iteration,starts_with("dQALY"),starts_with("dCOST"),takeup_baseline,takeup_reformed) %>% 
    left_join(df_psa_params, "iteration") %>% 
      rename(psa_id = iteration) %>% 
  select(-pop_fpl,-plan_type,-plan_premium,-baseline_premium,-contains("_coef")) %>% 
  mutate(dQALY_NOSUBSIDY = 0, dCOST_NOSUBSIDY=0)


df_psa_full  %>% 
  summarize_at(vars(-psa_id),mean)
parm1<-'gov_incidence'
parm2<-'eta'
range1<-c(0,1)
range2<-c(0.5,1)


```

```{r, eval = FALSE}
library(patchwork)
p1 <- OneWaySA(indata = df_psa_full, outcome = "NHB", parm = parm1, range = range1, lambda = 1/.9) + 
  ggtitle("One Way Sensitivity Analysis:\nGovernment Incidence of Uncompensated Care\nMVPF Benchmark = 0.9") + 
  theme(legend.position = "none") +
 xlim(c(0,1.3))  + ylab("E[NWB]") + ylim(c(-3,1))

p2 <- OneWaySA(indata = df_psa_full, outcome = "NHB", parm = parm1, range = range1, lambda = 1/.2) + 
  ggtitle("One Way Sensitivity Analysis:\nGovernment Incidence of Uncompensated Care\nMVPF Benchmark = 0.2") + 
  theme(legend.position = "none") +
 xlim(c(0,1.3))  + ylab("E[NWB]") + ylim(c(-3,1))

p1 + p2 

```

```{r, eval = FALSE}
TwoWaySA(indata=df_psa_full,outcome="NHB",parm1,parm2,range1,range2,lambda = 1/.9) +
  ggthemes::theme_tufte() + 
  scale_fill_manual(values = c("black","white","grey")) + 
  ggtitle("Two-Way Sensitivity Analysis:\nGovernment Incidence of Uncompensated Care\nIncidence on Affluent (eta = 0.5) vs. \nLow-Income (eta = 1)\nMVPF Benchmark = 0.9")  +
  ggthemes::theme_clean()

```


```{r, eval = FALSE}
CEAC<-function(lambda_range,indata){
  # Get Outcome
  lhs <- indata %>% select(contains("dQALY"),contains("dCOST")) %>% 
    mutate(psa_id = row_number()) %>% 
    reshape2::melt(id.vars='psa_id') %>% 
    tidyr::separate(variable,c("outcome","strategy"),"_") %>% 
    reshape2::dcast(psa_id+strategy~outcome) 

  # Get Parameters
  rhs <- indata %>% select(-contains("dQALY"),-contains("dCOST"),-psa_id) 
  
  # Map to existing code inputs
  Strategies <- unique(lhs$strategy)
  Parms <- rhs %>% tbl_df()
  Outcomes <- lhs %>% select(strategy,psa_id,contains("dCOST"),contains("dQALY")) 
  
  # Outcomes must be ordered in a way that for each strategy the cost must appear first then the effectiveness
  lambda<- lambda_range
  
  NHB <- array(0, dim=c(dim(Outcomes)[1],length(Strategies))) # Matrix to store NHB for each strategy
  colnames(NHB)<-Strategies
  CEA<-array(0,dim=c(length(lambda),length(Strategies)))
  
  #
  NHB <- lambda %>% purrr::map(~(Outcomes$dQALY-Outcomes$dCOST * .x)) %>% do.call("cbind",.)  
  colnames(NHB) <- paste0("lambda_",lambda)
  NHB <- data.frame(NHB)
  NHB$strategy <- Outcomes$strategy
  NHB$psa_id <- Outcomes$psa_id 
  NHB2 <- NHB %>% reshape2::melt(id.vars=c("strategy","psa_id"))  
  NHB2 <- NHB2 %>% split(NHB2$variable) 
  foo <- NHB2 %>% map2(.,names(.),~select(.x,-variable)) %>% 
    map2(.,names(.),~mutate(.x,lambda=as.numeric(gsub("lambda_","",.y)))) %>% 
    map2(.,names(.),~mutate(.x,NHB="NHB"))  %>% 
    map2(.,names(.),~reshape2::dcast(.x,psa_id~NHB+strategy)) 
  
  Optimal <- CEA <- list()
  for (i in names(foo)) {
    max.temp <- foo[[i]][,-1] %>% apply(.,1,max)
    Optimal[[i]] <- foo[[i]][,-1] %>% tbl_df() %>% mutate_all(funs(as.integer(.==max.temp)))
    CEA[[i]] <- colMeans(Optimal[[i]])
  }
  CEA <- do.call("rbind",CEA) %>% tbl_df() %>% mutate(lambda=as.numeric(gsub("lambda_","",names(foo))))
  colnames(CEA)<- gsub("NHB_","",colnames(CEA))
  
  CEAC<-reshape2::melt(CEA, id.vars = "lambda") 
  library(directlabels)
  txtsize<-12
  
  CEAC <- CEAC %>% mutate(variable = paste0("  ",variable,"  "))
  
  p <- ggplot(data = CEAC, aes(x = lambda, y = value, color = variable)) +
    geom_point() +
    geom_line() +
    ggtitle("Cost-Effectiveness Acceptability Curves") + 
    scale_colour_hue("Strategies: ",l=50) +
    #scale_x_continuous(breaks=number_ticks(6))+
    xlab(expression("Willingness to Adopt "(lambda))) +
    ylab("Pr Cost-Effective") +
    theme_bw() +
    theme(legend.position="bottom",legend.title=element_text(size = txtsize),
          legend.key = element_rect(colour = "black"),
          legend.text = element_text(size = txtsize),
          title = element_text(face="bold", size=15),
          axis.title.x = element_text(face="bold", size=txtsize),
          axis.title.y = element_text(face="bold", size=txtsize),
          axis.text.y = element_text(size=txtsize),
          axis.text.x = element_text(size=txtsize))+scale_colour_grey(start = .5, end = 1)+
    geom_dl(aes(label = variable), method = list(dl.combine( "last.points"), cex = 0.8))
  return(p)
}

lambda_range <- seq(0.0,1.5,0.1)
CEAC(indata=df_psa_full,lambda_range=lambda_range) + 
  ggthemes::theme_clean() + 
  scale_x_continuous(breaks = lambda_range, limits = c(0,2)) + 
  theme(legend.position = "none") + 
  ggtitle("Policy Acceptability Curve") + 
  ylab("Pr(Cost-Effective)") + 
  geom_vline(aes(xintercept = c(0.2)), lty=2) + 
  annotate("text",x = 0.22, y = 0.25, label = "SWF\nBenchmark",hjust =0,size =2) +
  geom_vline(aes(xintercept = c(0.88)), lty=2) + 
  annotate("text",x = 0.86, y = 0.25, label = "EITC\nBenchmark",hjust =1,size = 2) +
    scale_colour_brewer(palette = "Set1")+
   scale_colour_grey( start = 0, end = .5, na.value = "red")



```

```{r, eval = FALSE}
TornadoDiag <- function(indata,outcome,lambda) { 
  # Get Outcome
  lhs <- indata %>% select(psa_id,contains("dQALY"),contains("dCOST")) %>% 
    mutate(psa_id = row_number()) %>% 
    reshape2::melt(id.vars='psa_id') %>% 
    tidyr::separate(variable,c("outcome","strategy"),"_") %>% 
    reshape2::dcast(psa_id+strategy~outcome) %>% 
    mutate(NHB = dQALY-dCOST * lambda , 
           NMB = dQALY*lambda - dCOST) 
  
  # Get Parameters
  rhs <- indata %>% select(-contains("dQALY"),-contains("dCOST"),
                               -contains("NMB"),-contains("NHB"),-psa_id)
  
  # Map to existing code inputs
  Strategies <- unique(lhs$strategy)
  Parms <- rhs %>% tbl_df()
  lhs$Y <- lhs[,outcome]
  Outcomes <- lhs %>% select(strategy,psa_id,Y) %>% 
    reshape2::dcast(psa_id~strategy,value.var="Y") %>% 
    select(-psa_id)
  
  # Find the Optimal 
  opt<-which.max(colMeans(Outcomes)); opt
  
  # calculate min and max vectors of the parameters (e.g., lower 2.5% and 97.5%)
  X <- as.matrix(Parms)
  y <- as.matrix(Outcomes[,opt])
  Y <- as.matrix(Outcomes)
  
  ymean <- mean(y)
  n <- nrow(Parms)
  nParams <- ncol(Parms)
  
  paramNames <- colnames(Parms)
  
  Parms.sorted <- apply(Parms,2,sort,decreasing=F)  #Sort in increasing order each column of Parms
  lb <- 2.5
  ub <- 97.5 
  Xmean <- rep(1,nParams) %*% t(colMeans(X))
  XMin <- Xmean
  XMax <- Xmean
  paramMin <- as.vector(Parms.sorted[round(lb*n/100),])
  paramMax <- as.vector(Parms.sorted[round(ub*n/100),])
  
  diag(XMin) <- paramMin
  diag(XMax) <- paramMax
  
  XMin <- cbind(1, XMin)
  XMax <- cbind(1, XMax)
  
  X <- cbind(1,X)
  B <- solve(t(X) %*% X) %*% t(X) %*% y # Regression for optimal strategy
  
  library(matrixStats)
  
  bigBeta <- solve(t(X) %*% X) %*% t(X) %*% Y # Regression for all strategies
  
  yMin <- rowMaxs(XMin %*% bigBeta - ymean)
  yMax <- rowMaxs(XMax %*% bigBeta - ymean)
  ySize <- abs(yMax - yMin) 
  
  rankY<- order(ySize)
  
  xmin <- min(c(yMin, yMax)) + ymean
  xmax <- max(c(yMin, yMax)) + ymean
  
  paramNames2 <- paste(paramNames, "[", round(paramMin,2), ",", round(paramMax,2), "]")
  
  strategyNames<-Strategies
  colfunc <- colorRampPalette(c("black", "white"))
  strategyColors <- colfunc(length(Strategies))
  
  ## Polygon graphs:
  nRect <- 0
  x1Rect <- NULL
  x2Rect <- NULL
  ylevel <- NULL
  colRect <- NULL
  
  for (p in 1:nParams){
    xMean <- colMeans(X)
    xStart = paramMin[rankY[p]]
    xEnd = paramMax[rankY[p]]
    xStep = (xEnd-xStart)/1000
    for (x in seq(xStart,xEnd, by = xStep)){
      #for each point determine which one is the optimal strategy
      xMean[rankY[p] + 1] <- x    # +1 moves beyond the constant
      yOutcomes <- xMean %*% bigBeta
      yOptOutcomes <- max(yOutcomes)
      yOpt <- strategyNames[which.max(yOutcomes)]
      if (x == xStart){
        yOptOld <- strategyNames[which.max(yOutcomes)]
        y1 <- yOptOutcomes
      }
      #if yOpt changes, then plot a rectangle for that region
      if (yOpt != yOptOld | x == xEnd){
        nRect <- nRect + 1
        x1Rect[nRect] <- y1
        x2Rect[nRect] <- yOptOutcomes
        ylevel[nRect] <- p
        colRect[nRect] <- yOptOld
        yOptOld <- yOpt
        y1 <- yOptOutcomes
      }
    }
  }
  
  txtsize <-8
  d=data.frame(x1=x2Rect, x2=x1Rect, y1=ylevel-0.4, y2=ylevel+0.4, t=colRect, r = ylevel)
  
  p <- ggplot(d, aes(xmin = x1, xmax = x2, ymin = y1, ymax = y2, fill = t)) +
    xlab(paste0("Expected ",outcome)) +
    ylab("Parameters") + 
    geom_rect()+
    theme_bw() + 
    scale_y_continuous(limits = c(0.5, nParams + 0.5),breaks=seq(1:ncol(Parms)), labels=paramNames2[rankY]) +
    scale_fill_grey(start = 0, end = .9)+
    geom_vline(xintercept=ymean, linetype="dotted") + 
    theme(legend.position="bottom",legend.title=element_text(size = txtsize),
          legend.key = element_rect(colour = "black"),
          legend.text = element_text(size = txtsize),
          title = element_text(face="bold", size=1),
          axis.title.x = element_text(face="bold", size=txtsize),
          axis.title.y = element_text(face="bold", size=txtsize),
          axis.text.y = element_text(size=txtsize),
          axis.text.x = element_text(size=txtsize))+ labs(fill="")
  return(p)
}
TornadoDiag(indata = df_psa_full %>% select(-contains("NOSUBSIDY")), outcome = "NHB",lambda = 0.88) + 
  xlab("Expected NWB")
```


```{r, eval = FALSE}

predict.ga <- function(object, n, n0, verbose = T){
  #### Function to compute the preposterior for each of the 
  #### basis functions of the GAM model.
  #### Inputs:
  #### - object: gam object
  #### - n: scalar or vector of new sample size to compute evsi on
  #### - n0: scalar or vector of effective prior sample size
  #### - verbose: Prints the variance reduction factor for each parameter
  
  ### Name of parameters
  names.data <- colnames(object$model)
  ### Create dataframe with parameter values
  data <- data.frame(object$model[,-1])
  ## Name columns of dataframe 
  colnames(data) <- names.data[-1]
  
  ### Number of parameters
  n.params <- ncol(data)
  
  ### Sanity checks
  if(!(length(n)==1 | length(n)==n.params)){
    stop("Variable 'n' should be either a scalar or a vector 
         the same size as the number of parameters")
  }
  if(!(length(n0)==1 | length(n0)==n.params)){
    stop("Variable 'n0' should be either a scalar or a vector 
         the same size as the number of parameters")
  }
  
  ### Make n & n0 consistent with the number of parameters
  if(length(n) == 1){
    n <- rep(n, n.params)
  }
  if(length(n0) == 1){
    n0 <- rep(n0, n.params)
  }
   
  ### Compute variance reduction factor
  v.ga <- sqrt(n/(n+n0))
  if (verbose){
    print(paste("Variance reduction factor =", round(v.ga, 3)))
  }
  
  ### Number of smoothers
  n.smooth <- length(object$smooth)
  ### Number of total basis functions
  n.colX <- length(object$coefficients)
  ### Number of observations 
  n.rowX <- nrow(object$model)
  
  ### Initialize matrix for preposterior of total basis functions
  X <- matrix(NA, n.rowX, n.colX)
  X[, 1] <- 1
  
  for (k in 1:n.smooth) { # k <- 1
    klab <- substr(object$smooth[[k]]$label, 1, 1)
    if (klab == "s"){
      Xfrag <- Predict.smooth.ga(object$smooth[[k]], data, v.ga[k])
    } else {
      Xfrag <- Predict.matrix.tensor.smooth.ga(object$smooth[[k]], data, v.ga)
    }
    X[, object$smooth[[k]]$first.para:object$smooth[[k]]$last.para] <- Xfrag
  }
  
  ### Coefficients of GAM model
  Beta <- coef(object)
  
  ### Compute conditional Loss
  Ltilde <- X %*% Beta
  
  return(Ltilde)
}

Predict.smooth.ga <- function (object, data, v.ga = 1) {
  #### Function to compute the preposterior for each of the 
  #### basis functions of a smooth for one parameter
  
  ### Produce basis functions for one parameter
  X <- PredictMat(object, data) # ‘mgcv’ version 1.8-17
  ## Number of observations
  n.obs <- nrow(X)
  
  ### Apply variance reduction to compute the preposterior 
  ### for each of the basis functions
  ## Vector of ones
  ones <- matrix(1, n.obs, 1)
  ## Compute phi on each of the basis function
  X.ga <- v.ga*X + (1-v.ga)*(ones %*% colMeans(X))
  
  return(X.ga)
}

Predict.matrix.tensor.smooth.ga <- function (object, 
                                             data, 
                                             v.ga = rep(1, ncol(data))){
  #### Function to compute the preposterior for each of the 
  #### basis functions for one or more parameters and calculates
  #### the tensor product if more than one parameter is selected
  #### (Heavily based on function Predict.matrix.tensor.smooth from
  #### mgcv package)
  
  m <- length(object$margin)
  X <- list()
  for (i in 1:m) { # i <- 1
    term <- object$margin[[i]]$term
    dat <- list()
    for (j in 1:length(term)) { # j <- 1
      dat[[term[j]]] <- data[[term[j]]]
    }
    X[[i]] <- if (!is.null(object$mc[i])) # before: object$mc[i]
      PredictMat(object$margin[[i]], dat, n = length(dat[[1]])) # ‘mgcv’ version 1.8-17
    else Predict.matrix(object$margin[[i]], dat)
    n.obs <- nrow(X[[i]])
  } # end for 'i'
  mxp <- length(object$XP)
  if (mxp > 0) 
    for (i in 1:mxp) if (!is.null(object$XP[[i]])) 
      X[[i]] <- X[[i]] %*% object$XP[[i]]
  
  ### Apply variance reduction to compute the preposterior 
  ### for each of the basis functions
  ## Vector of ones
  ones <- matrix(1, n.obs, 1)
  ## Initialize and fill list with preposterior of basis functions 
  ## for each parameter
  X.ga <- list()
  for (i in 1:m) { # i <- 1
    X.ga[[i]] <- v.ga[i]*X[[i]] + (1-v.ga[i])*(ones %*% colMeans(X[[i]]))
  }
  
  ### Compute tensor product
  T.ga <- tensor.prod.model.matrix(X.ga) # ‘mgcv’ version 1.8-17
  
  return(T.ga)
}
```

```{r, eval = FALSE}
indata <- df_psa_full %>% select(-contains("LOWSUBSIDY"))

evppi.res <- list()

for (lambda in seq(0.1,1.5,0.1)) {
  # Get Outcome
  cat(lambda)
  
  lhs <- indata %>% select(psa_id,contains("dQALY"),contains("dCOST")) %>%
    mutate(psa_id=row_number()) %>% 
    reshape2::melt(id.vars='psa_id') %>%
    tidyr::separate(variable,c("outcome","strategy"),"_") %>%
    reshape2::dcast(psa_id+strategy~outcome) %>%
    mutate(NHB = dQALY- dCOST * lambda ,
           NMB = dQALY - dCOST *lambda)
  nmb <- lhs %>% select(psa_id,strategy,NMB) %>% 
    mutate(variable="NMB") %>% reshape2::dcast(psa_id~variable+strategy,value.var="NMB") %>% 
    select(-psa_id)
  
  # Get Parameters
  theta <- indata %>% select(-contains("dQALY"),-contains("dCOST"),
                             -contains("NMB"),-contains("NHB"),-psa_id)
  
  ## Number of simulations
  n.sim        <- nrow(nmb)
  ## Number of strategies
  n.strategies <- ncol(nmb)
  
  ### Load required packages and functions
  ## For column and row stats
  library(matrixStats)  
  ## To fit spline models
  library(mgcv) # version 1.8-17
  ## Functions to calculate the conditional loss by computing the 
  ## preposterior of each of the basis functions of the GAM model
  #source("../reference/metamodeling/supplemental-material/GA_functions.R")
  
  ### Find optimal strategy (d*) based on the highest expected NMB
  d.star <- which.max(colMeans(nmb))
  d.star
  
  ### Define the Loss matrix
  loss <- nmb - nmb[, d.star]
  
  ### EVPI
  evpi <- mean(rowMaxs(as.matrix(loss)))
  evpi
  
  #========================#
  #### Single parameter ####
  #========================#
  ### Generate linear metamodel of one parameter for each opportunity loss
  ## Selected parameter for EVPPI & EVSI
  
  evppi.res[[paste0(lambda)]] <- data.frame(parameter = names(theta)) %>% mutate(evppi = NA)
  
  for (sel.param in seq(ncol(theta))) {
    
    cat("\n")
    cat(paste0("  ",sel.param))
    
    lmm1 <- gam(as.formula(paste("loss[, 1] ~ s(", colnames(theta)[sel.param], ")")),
                data = theta)
    lmm2 <- gam(as.formula(paste("loss[, 2] ~ s(", colnames(theta)[sel.param], ")")),
                data = theta)
    #lmm3 <- gam(as.formula(paste("loss[, 3] ~ s(", colnames(theta)[sel.param], ")")),
                #data = theta)
    #lmm4 <- gam(as.formula(paste("loss[, 4] ~ s(", colnames(theta)[sel.param], ")")),
                #data = theta)  
    
    #### Compute EVPPI on one parameter ####
    ## Compute estimated losses
    loss.hat <- cbind(lmm1$fitted, lmm2$fitted)#, lmm3$fitted,lmm4$fitted)
    
    ### Apply EVPPI equation
    evppi.res[[paste0(lambda)]][sel.param,]$evppi <- mean(rowMaxs(loss.hat))
    cat("\n")
  }
} 



  evppi.res %>% 
  bind_rows(.id = "lambda") %>% 
  #filter(lambda>0.8 & lambda<1.2) %>% 
  ggplot(aes(x = lambda, y = evppi, group = parameter, colour = parameter)) + geom_line() + 
    ggthemes::theme_clean()

```

